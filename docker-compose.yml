version: '3.8'

services:

  # === POSTGRES ===
  postgres:
    image: postgres:15
    container_name: postgres
    environment:
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
      POSTGRES_DB: sportdb
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    command: >
      postgres -c wal_level=logical -c max_replication_slots=4 -c max_wal_senders=4
    networks:
      - airflow_net

  adminer:
    image: adminer
    restart: always
    ports:
      - 8080:8080
    networks:
      - airflow_net

  # === MONITORING ===
  prometheus:
    image: prom/prometheus
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
    networks:
      - airflow_net

  grafana:
    image: grafana/grafana
    container_name: grafana
    ports:
      - "3000:3000"
    depends_on:
      - prometheus
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning
      - ./grafana/dashboards:/var/lib/grafana/dashboards
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    networks:
      - airflow_net

  postgres-exporter:
    image: prometheuscommunity/postgres-exporter
    container_name: postgres_exporter
    environment:
      DATA_SOURCE_NAME: "postgresql://user:password@postgres:5432/sportdb?sslmode=disable"
    ports:
      - "9187:9187"
    depends_on:
      - postgres
    networks:
      - airflow_net

  airflow-exporter:
    build:
      context: .
      dockerfile: Dockerfile.airflow_exporter
    container_name: airflow_exporter
    ports:
      - "9112:9112"
    environment:
      - AIRFLOW_URL=http://airflow-webserver:8080
      - AIRFLOW_USER=admin
      - AIRFLOW_PASSWORD=admin
    depends_on:
      - airflow-webserver
    networks:
      - airflow_net

  node-exporter:
    image: prom/node-exporter
    container_name: node_exporter
    ports:
      - "9100:9100"
    networks:
      - airflow_net

  # === AIRFLOW ===
  airflow-init:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    depends_on:
      - postgres
    env_file: .env
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://user:password@postgres:5432/sportdb
      - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW_SECRET_KEY}
      - AIRFLOW__METRICS__STATSD_ON=True
      - AIRFLOW__METRICS__STATSD_HOST=airflow-exporter
      - AIRFLOW__METRICS__STATSD_PORT=9125
    volumes:
      - ./dags:/opt/airflow/dags
      - ./data:/opt/airflow/data
      - ./spark-jobs:/opt/airflow/spark-jobs
    restart: on-failure
    networks:
      - airflow_net
    entrypoint: >
      bash -c "
        airflow db init &&
        airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com
      "

  airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    depends_on:
      - postgres
      - airflow-init
    env_file: .env
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://user:password@postgres:5432/sportdb
      - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW_SECRET_KEY}
      - AIRFLOW__METRICS__STATSD_ON=True
      - AIRFLOW__METRICS__STATSD_HOST=airflow-exporter
      - AIRFLOW__METRICS__STATSD_PORT=9125
      - AIRFLOW__API__AUTH_BACKEND=airflow.api.auth.backend.basic_auth
    volumes:
      - ./dags:/opt/airflow/dags
      - ./data:/opt/airflow/data
      - ./soda:/opt/airflow/soda
      - /var/run/docker.sock:/var/run/docker.sock  
      - ./spark-jobs:/opt/airflow/spark-jobs

    ports:
      - "8081:8080"
    command: webserver
    networks:
      - airflow_net

  airflow-scheduler:
    build:
      context: . 
      dockerfile: Dockerfile.airflow
    depends_on:
      - airflow-webserver
    env_file: .env
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://user:password@postgres:5432/sportdb
      - AIRFLOW__WEBSERVER__SECRET_KEY=${AIRFLOW_SECRET_KEY}
      - AIRFLOW__METRICS__STATSD_ON=True
      - AIRFLOW__METRICS__STATSD_HOST=airflow-exporter
      - AIRFLOW__METRICS__STATSD_PORT=9125
    volumes:
      - ./dags:/opt/airflow/dags
      - ./data:/opt/airflow/data
      - ./soda:/opt/airflow/soda
      - /var/run/docker.sock:/var/run/docker.sock  
      - ./spark-jobs:/opt/airflow/spark-jobs
    command: scheduler
    networks:
      - airflow_net

  enrich_rh:
    build:
      context: .
      dockerfile: Dockerfile.enrich
    volumes:
      - ./data:/app/data
      - ./.env:/app/.env
    depends_on:
      - postgres
    restart: "no"
    networks:
      - airflow_net

  # === DEBEZIUM + REDPANDA ===
  redpanda:
    image: redpandadata/redpanda:v23.1.10
    container_name: redpanda
    command:
      - redpanda
      - start
      - --overprovisioned
      - --smp 1
      - --memory 1G
      - --reserve-memory 0M
      - --node-id 0
      - --check=false
      - --kafka-addr PLAINTEXT://0.0.0.0:9092,OUTSIDE://0.0.0.0:29092
      - --advertise-kafka-addr PLAINTEXT://redpanda:9092,OUTSIDE://localhost:29092
    ports:
      - "9092:9092"     # Pour les conteneurs internes
      - "29092:29092"   # Pour ton host Windows (tests extérieurs)
    networks:
      - airflow_net


  redpanda-console:
    image: docker.redpanda.com/redpandadata/console:latest
    container_name: redpanda_console
    depends_on:
      - redpanda
    ports:
      - "8082:8080"  # <-- C'est l'interface web
    environment:
      - KAFKA_BROKERS=redpanda:9092
    networks:
      - airflow_net

  connect:
    image: debezium/connect:2.6
    container_name: debezium
    depends_on:
      - postgres
      - redpanda
    ports:
      - "8083:8083"
    environment:
      BOOTSTRAP_SERVERS: redpanda:9092
      GROUP_ID: 1
      CONFIG_STORAGE_TOPIC: debezium_config
      OFFSET_STORAGE_TOPIC: debezium_offsets
      STATUS_STORAGE_TOPIC: debezium_statuses
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_REST_ADVERTISED_HOST_NAME: connect
      CONNECT_PLUGIN_PATH: /kafka/connect,/debezium-plugins
    networks:
      - airflow_net

  register-debezium:
    image: curlimages/curl
    container_name: register_debezium
    depends_on:
      - connect
    volumes:
      - ./scripts:/scripts
    entrypoint: [ "sh", "/scripts/register-debezium.sh" ]
    networks:
      - airflow_net



  # === KAFKA CONSUMER TRIGGER AIRFLOW ===
  kafka-listener:
    build:
      context: ./kafka-listener
    container_name: kafka-listener
    depends_on:
      - redpanda
      - connect
      - airflow-webserver
    environment:
      - KAFKA_TOPIC=pgserver1.public.activites
      - KAFKA_BOOTSTRAP_SERVERS=redpanda:9092
      - AIRFLOW_API=http://airflow-webserver:8080/api/v1/dags
      - AIRFLOW_USER=admin
      - AIRFLOW_PASSWORD=admin
      - SLACK_WEBHOOK_URL=${SLACK_WEBHOOK_URL}  #  Injectée depuis .env
    volumes:
      - ./kafka-listener:/app
      - ./.env:/app/.env                         
    command: python /app/listener.py   
    networks:
      - airflow_net          
    env_file:
      - .env

  # === SPARK
  spark:
    build:
      context: .
      dockerfile: Dockerfile.spark
    image: custom-spark:delta 
    container_name: spark
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_DIRS=/tmp/spark
    ports:
      - "7077:7077"
      - "8088:8080"
    volumes:
      - ./spark-jobs:/opt/spark-apps
      - ./data:/opt/spark-data
      - ./spark-data/export:/opt/spark-data/export
    command: /opt/bitnami/scripts/spark/run.sh
    networks:
      - airflow_net


  spark-worker:
    image: bitnami/spark:3.5
    container_name: spark_worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
    depends_on:
      - spark
    volumes:
      - ./spark-jobs:/opt/spark-apps
      - ./data:/opt/spark-data
    networks:
      - airflow_net

volumes:
  postgres_data:
  grafana_data:

networks:
  airflow_net:
